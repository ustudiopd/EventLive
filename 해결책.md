좋아요. 지금 구조를 **Postgres Changes → Broadcast 중심**으로 바꾸는 것을 전제로, 현재 프로젝트를 점검하고 바로 적용 가능한 설계/코드/체크리스트를 정리해 드릴게요. 아래는 **웨비나 실시간 채팅/설문/퀴즈/추첨**에 맞춘 **Broadcast 우선(WS) + DB 영속 + 증분 폴백** 아키텍처입니다.

---

## 0) 요약 (TL;DR)

* **전달(transport)**: Supabase **Broadcast**를 기본으로 사용해 지연·부하·RLS 영향 최소화.
* **근본 원인 회피**: 수동 재연결/중복 unsubscribe 경쟁 상태 제거(현재 리포트에서 지적된 루프 원인) — 채널은 1개만, cleanup과 재연결은 **단일 책임**으로.  
* **정합성**: DB는 **단일 진실 소스(SOT)**. 송신 시 **API→DB insert 성공** 후 **Broadcast 전파**(낙관적 UI 업데이트는 `client_msg_id`로 정합성 맞춤). 
* **폴백**: WS 3회 실패 또는 `TIMED_OUT` 연속 시 **증분 폴링**(`?after=<lastId>`) 활성화. WS 복구 시 자동 해제. 
* **권한/보안**: Broadcast는 RLS를 타지 않으므로 **행위 권한**은 API/DB RLS로 강제, **호스트 제어 이벤트**는 서명/토큰으로 검증. (RLS 단순화/무한재귀 이슈는 별도 가이드 준수)  

---

## 1) 현재 코드와 충돌 요인 정리

* **이중 재연결 충돌**: Supabase SDK의 자동 재연결과 우리 쪽 수동 재연결 타이머가 서로 엇갈리며 `SUBSCRIBED→CLOSED/TIMED_OUT→재시도` 루프 유발. **중복 정리/생성** 경쟁까지 겹치며 악화.   
* **비동기 정리 타이밍**: `unsubscribe()` 완료 전 새 채널 생성 → 다중 채널 공존/루프. 정리는 **await** 보장 후 다음 단계로.  
* **Strict Mode/다중 마운트** 고려 필요: 개발 모드 이펙트 2회 실행 → 중복 구독. **단일 인스턴스 보장**과 의존성 최소화가 핵심. 

> 위 문제는 **postgres_changes** 구독일 때 특히 RLS/트리거/조인비용까지 겹쳐 체감이 커졌습니다. **Broadcast**로 교체하면 RLS·트리거 부담 없이 WS 레이어만 안정화하면 됩니다. 현 리포트에서 이미 폴백·증분 폴링·타이머 정리·단일 인스턴스를 적용해 두신 점은 큰 장점입니다.  

---

## 2) 이벤트 모델(추천)

**채널 이름(1개):** `webinar:${webinarId}`
**이벤트 타입:**

* `chat:new`, `chat:update`, `chat:delete`
* `quiz:open`, `quiz:answer`, `quiz:close`
* `poll:open`, `poll:vote`, `poll:close`
* `raffle:start`, `raffle:draw`, `raffle:done`
* `typing`, `presence:*`(SDK)

**공통 Envelope:**

```ts
type BroadcastEnvelope<T = any> = {
  v: 1;                  // schema version
  t: string;             // event type e.g. 'chat:new'
  mid?: string;          // client_msg_id (중복 방지/낙관적 교체)
  sid: string;           // sender user id
  ts: number;            // Date.now() 송신 시각
  payload: T;            // 도메인 데이터
  sig?: string;          // (선택) 호스트 제어 이벤트 서명
};
```

> 이미 도입하신 `client_msg_id`를 그대로 Envelope에 넣고, 수신 측은 **mid 기준 중복/역전 방지** 처리합니다. 

---

## 3) 구독 코드 (Broadcast로 교체)

아래는 **React + Next.js(App Router)** 클라이언트 코드 예시입니다.
핵심 포인트: **단일 채널**, **status별 타이머/폴백/cleanup 일원화**, **중복 구독 금지**.

```tsx
'use client'
import { useEffect, useMemo, useRef } from 'react'
import { createClient } from '@supabase/supabase-js'

export function useWebinarRealtime({
  webinarId,
  currentUserId,
  onEvent,
  onReconnectState,
}: {
  webinarId: string
  currentUserId: string
  onEvent: (env: BroadcastEnvelope) => void
  onReconnectState?: (s: 'ok' | 'retrying' | 'fallback') => void
}) {
  // 1) Supabase 클라이언트 고정 (이미 적용하신 패턴과 동일)
  const supabase = useMemo(() => createClient(
    process.env.NEXT_PUBLIC_SUPABASE_URL!,
    process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!
  ), [])

  // 2) refs
  const channelRef = useRef<ReturnType<typeof supabase.channel> | null>(null)
  const reconnectTRef = useRef<ReturnType<typeof setTimeout> | null>(null)
  const fallbackTRef  = useRef<ReturnType<typeof setTimeout> | null>(null)
  const reconnectTries = useRef(0)
  const isMounted = useRef(false)

  useEffect(() => {
    if (!webinarId || !currentUserId) return
    isMounted.current = true

    // 기존 채널 있으면 정리(완료를 '반드시' 기다림)  ← 경쟁 방지
    const existing = supabase.getChannels().find(ch => ch.topic === `realtime:webinar:${webinarId}`)
    const cleanupExisting = existing
      ? existing.unsubscribe().then(() => supabase.removeChannel(existing))
      : Promise.resolve()

    let cancelled = false

    cleanupExisting.then(() => {
      if (cancelled) return

      // 3) 단일 채널 생성
      const ch = supabase.channel(`webinar:${webinarId}`, {
        config: {
          broadcast: { self: false },   // 내 메시지는 낙관적 업데이트로 처리
          presence: { key: currentUserId },
        },
      })

      // 4) Broadcast 수신
      ch.on('broadcast', { event: '*' }, (payload: any) => {
        const env = payload?.payload as BroadcastEnvelope | undefined
        if (env?.v !== 1 || !env?.t) return
        onEvent(env)
      })

      // 5) Presence (선택)
      ch.on('presence', { event: 'sync' }, () => {
        // 필요 시 ch.presenceState()로 참가자 목록/카운트 추출
      })

      // 6) 구독 + 상태 처리 (재연결/폴백 일원화)
      ch.subscribe(async (status) => {
        if (status === 'SUBSCRIBED') {
          reconnectTries.current = 0
          onReconnectState?.('ok')
          // 재시도 타이머들 정리
          if (reconnectTRef.current) { clearTimeout(reconnectTRef.current); reconnectTRef.current = null }
          if (fallbackTRef.current)  { clearTimeout(fallbackTRef.current);  fallbackTRef.current = null }
        } else if (status === 'TIMED_OUT' || status === 'CHANNEL_ERROR' || status === 'CLOSED') {
          reconnectTries.current += 1
          const n = reconnectTries.current
          if (n >= 3) {
            // 폴백: 30초 후 재도전 + 즉시 폴링 전환
            onReconnectState?.('fallback')
            if (reconnectTRef.current) { clearTimeout(reconnectTRef.current); reconnectTRef.current = null }
            if (!fallbackTRef.current) {
              fallbackTRef.current = setTimeout(() => {
                reconnectTries.current = 0
                ch.unsubscribe().then(() => {
                  supabase.removeChannel(ch)
                  // 재구독 (cleanup → subscribe)
                  channelRef.current = null
                  // 상위에서 hook 재실행 or 별도 state로 재생성 트리거
                })
                fallbackTRef.current = null
              }, 30000)
            }
          } else {
            // 지수 백오프 재시도 (SDK 자동 재조인과 '중복'으로 하지 않도록, 여기서는 재구독 대신 '대기'만)
            onReconnectState?.('retrying')
            const delay = Math.min(1000 * (2 ** (n - 1)), 8000)
            if (!reconnectTRef.current) {
              reconnectTRef.current = setTimeout(() => {
                // 핵심: 여기서 수동 re-subscribe를 새로 만들지 않음 (SDK가 재시도)
                reconnectTRef.current = null
              }, delay)
            }
          }
        }
      })

      channelRef.current = ch
    })

    // cleanup
    return () => {
      cancelled = true
      isMounted.current = false
      if (reconnectTRef.current) { clearTimeout(reconnectTRef.current); reconnectTRef.current = null }
      if (fallbackTRef.current)  { clearTimeout(fallbackTRef.current);  fallbackTRef.current = null }
      const ch = channelRef.current
      channelRef.current = null
      if (ch) {
        ch.unsubscribe().then(() => { supabase.removeChannel(ch) })
      }
    }
  // 의존성 최소화(단일 인스턴스/중복 구독 방지)
  }, [webinarId, currentUserId])

  // 7) 송신 (상세는 아래)
  const send = async (event: string, env: BroadcastEnvelope['payload'], mid?: string) => {
    const ch = channelRef.current
    if (!ch) throw new Error('channel not ready')
    const envelope: BroadcastEnvelope = { v: 1, t: event, mid, sid: currentUserId, ts: Date.now(), payload: env }
    const res = await ch.send({ type: 'broadcast', event, payload: envelope })
    return res // {status: 'ok' | 'timed_out' | 'error' ...}
  }

  return { send }
}
```

> 위 패턴은 **기존 수동 재연결 루프/중복 정리** 문제를 피하고, SDK 자동 재시도와 **충돌하지 않도록** 설계되어 있습니다(“정리는 cleanup만, 재시도는 상태 통지만”). 이는 리포트에서 지적된 경쟁 상태/중복 정리 문제를 원천 봉쇄합니다.  

---

## 4) 송신 경로 & DB 영속(권장 패턴)

1. **사용자 메시지 전송(채팅)**

   * **API 먼저**: `/api/webinars/[id]/messages`에 POST → DB `messages` insert 성공 → **서버에서** 응답 직후 클라이언트가 Broadcast 전파
   * UI는 **낙관적**으로 먼저 반영(`client_msg_id`), 응답 수신 시 DB row id/시각으로 교체
   * `client_msg_id`로 중복/다중 탭 정합성 맞춤 (이미 도입하심) 

```ts
// 예시: 채팅 전송
const mid = crypto.randomUUID()
optimisticAdd(mid, content) // UI 즉시 반영

// 1) DB insert
const res = await fetch(`/api/webinars/${webinarId}/messages`, {
  method: 'POST',
  body: JSON.stringify({ content, client_msg_id: mid }),
  headers: { 'Content-Type': 'application/json' }
})
if (!res.ok) { optimisticRevert(mid); throw new Error('send failed') }
const { row } = await res.json()

// 2) Broadcast (낙관적과 동일 mid로 송신)
await realtime.send('chat:new', { id: row.id, content: row.content, created_at: row.created_at }, mid)
```

2. **호스트 제어 이벤트(퀴즈/설문/추첨)**

   * 브라우저에서 `channel.send`를 할 순 있지만, **권한 남용** 위험이 큼
   * **권장**: **서버 API**(Edge/Serverless)가 **권한 검증 후** DB에 이벤트 로그를 남기고, 성공 응답과 함께 **서명된 payload**(HMAC 등)를 내려주면, **클라이언트가 Broadcast**(혹은 서버가 Pusher 역할을 수행)
   * 수신자는 **서명 검증**해 신뢰되는 제어 이벤트만 반영
   * DB 로그는 폴백·재접속 시 **재생(replay)** 용도로 활용

> Broadcast는 DB RLS를 거치지 않으므로, **쓰기 권한/역할 검증은 반드시 API에서**. RLS 단순화/무한재귀 이슈도 서버 측 권한 확인/토큰 클레임 활용 방향과 궁합이 좋습니다.   

---

## 5) 폴백(증분) 폴링

* WS 상태가 `TIMED_OUT/CLOSED` **3회**면 폴백 가동 → `/api/webinars/[id]/messages?after=<lastId>` 증분 로드
* WS가 `SUBSCRIBED`로 복구되면 폴백 **즉시 중단**
* **지터 + 가시성/온라인** 고려로 과부하 줄이기(이미 구현 방향 OK)  

API 예시(이미 계획하신 패턴과 동일):

```ts
// GET /api/webinars/[webinarId]/messages?after=12345
// 서버에서 webinar 권한 검증 + RLS 우회(관리자 키)로 select
// after 있으면 gt(id, after) 조건으로 증분
```

> 증분 폴링 라우트와 클라이언트 폴백 로직은 이미 문서에 잘 정리되어 있습니다. 이 흐름을 Broadcast 교체 후에도 그대로 사용하세요. 

---

## 6) 권한/보안

* **채널 참가**: Supabase JWT 필요(미인증 접속 차단).
* **메시지/제어 이벤트**:

  * 일반 메시지: **API/DB RLS**로 권한 강제(등록자만 insert 등). RLS 단순화 전략을 점진 적용. 
  * 호스트 제어 이벤트: **서명된 payload** 또는 **서버 API만이 Broadcast 가능**하도록 설계. 수신 시 **검증 실패 이벤트는 무시**.
* **프로필/슈퍼어드민 조회**: 클라이언트 직접 select 금지 → **서버 API/JWT 클레임으로 대체**(무한재귀/성능 이슈 회피).  

---

## 7) 성능·안정성 체크리스트

* [ ] **단일 채널**: `webinar:${id}` 1개만 사용(메시지/제어/타이핑 모두 이벤트 분기로 처리)
* [ ] **cleanup 보장**: `await unsubscribe()` 이후 `removeChannel()`(경쟁 방지) 
* [ ] **의존성 최소화**: supabase/핵심 핸들러는 `useMemo`/`useRef` 고정. **Chat/QA 단일 인스턴스** 유지. 
* [ ] **타이머 추적**: 모든 재시도·폴백 타이머 ref 관리 + cleanup에서 전부 해제. 
* [ ] **Strict Mode**: 개발 모드 이펙트 두 번 실행 대비, **중복 연결 가드**(ref 플래그) 적용. 
* [ ] **낙관적 업데이트**: `client_msg_id` 매칭으로 중복/역전 방지(이미 도입). 
* [ ] **관측지표**: 구독 성공률/폴백 빈도/에러율/응답시간 알림. 

---

## 8) 실제 교체 Diff 포인트 (파일 기준)

* **`components/webinar/Chat.tsx`**

  * `.on('postgres_changes', …)` → **삭제**
  * `supabase.channel('webinar:${id}', { config: { broadcast…, presence… }})`로 **단일 채널 생성**
  * `channel.on('broadcast', { event: '*' }, handler)`로 **수신 전환**
  * `handleSend()`는 **API→DB insert 성공 후** `channel.send('chat:new', …)`로 전파
  * 기존 재연결/타이머 로직은 **상태 통지 + cleanup 일원화** 형태로 간소화(중복 unsubscribe 금지)  
* **`app/(webinar)/webinar/[id]/components/WebinarView.tsx`**

  * Chat/QA **단일 인스턴스 보장** 패턴 유지(모바일/데스크톱 공유). 
* **`app/api/webinars/[id]/messages/route.ts`**

  * **증분 폴링** 쿼리(`?after=`) 유지/강화, insert 성공 후 응답 반환. 
* **`lib/supabase/client.ts`**

  * 브라우저 전용 클라이언트 **고정** + 토큰 주입 유지. (서버는 Admin client로 권한 검증) 

---

## 9) 브라우저/배포 환경 팁 (Chrome/Edge 공통)

* **다중 탭**: 동일 사용자 다중 탭에서 이벤트 중복 처리 방지 → `client_msg_id` 기반 **dedup cache**(Map with TTL) 권장.
* **CSP/프록시**: WebSocket 차단/지연이 없는지(회사망/확장프로그램) 점검.
* **Next.js**: 실수로 SSR 경로에서 브라우저 클라이언트를 만들지 않도록 주의(클라이언트 전용).
* **가시성**: 탭 비활성 시 폴백 간격을 늘려 배터리/부하 감소.

---

## 10) 마이그레이션 순서 (권장)

1. **채널 전환**: `postgres_changes` 리스너 제거 → **Broadcast 채널**로 교체
2. **송신 경로**: **API→DB insert→Broadcast** 순서 확정(낙관적 UI 포함)
3. **폴백**: 증분 폴링 라우트 고정/테스트(WS 복구 시 자동 해제) 
4. **권한**: 호스트 제어 이벤트 **서명/검증** 추가, RLS 단순화 전략 1단계부터 적용(SELECT/INSERT 단순화) 
5. **관측/알림**: 구독 성공률·폴백 빈도 임계치 알림 세팅 

---

### 마무리

지금 겪고 계신 `SUBSCRIBED → CLOSED/TIMED_OUT → 재시도` 반복은 **수동 재연결/중복 정리 경쟁**과 맞물려 확대된 면이 큽니다. 위처럼 **Broadcast 단일 채널**로 단순화하고, **cleanup만이 채널 생명주기를 제어**하도록 바꾸면 루프는 멈추고, 설문/퀴즈/추첨도 **저지연**으로 안정화됩니다.
적용하면서 막히는 부분(특히 호스트 제어 이벤트 서명/검증, 폴백 증분 API)은 바로 보여주시면 코드까지 맞춰 드릴게요.
