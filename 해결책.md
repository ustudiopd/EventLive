원인과 증상이 딱 맞아 떨어집니다. **구독 → 즉시 해제(CLOSED) → 재시도 → 또 해제…** 가 반복되는 건, 코드 내부의 **재연결 타이머·cleanup·중복 인스턴스**가 서로 충돌하면서 스스로 연결을 끊고 다시 붙는 루프에 들어갔기 때문입니다. 아래 네 가지가 핵심 원인입니다.

---

## 왜 SUBSCRIBED 직후 CLOSED로 떨어질까?

1. **재연결 타이머 ↔ cleanup 경쟁 상태**
   `setReconnectKey` 같은 상태 변경으로 `useEffect`가 다시 돌면서 먼저 **cleanup이 실행(=unsubscribe)** 됩니다. 그런데 `setTimeout` 안에서도 채널 정리를 또 하므로 **중복 정리 → CLOSED** 가 바로 발생하고, 그게 다시 재연결을 유발하는 **루프**가 됩니다. 

2. **타이머 미정리**
   `setTimeout` 핸들이 ref로 보관·취소되지 않아, 컴포넌트가 재마운트/언마운트/재실행될 때 **고아(Orphan) 타이머**가 뒤늦게 발화 → 이미 닫힌 채널을 또 닫거나, 새 채널을 또 만들면서 **연결 불안정**을 키웁니다. 

3. **Chat 컴포넌트 중복 마운트**
   모바일/데스크톱 레이아웃에서 Chat을 각각 렌더링하면 **같은 webinarId로 두 개의 구독**이 붙습니다. 하나가 언마운트되며 cleanup이 돌면 다른 쪽 채널에도 영향을 주고, 로그에 **SUBSCRIBED → 해제 → 재시도**가 엮여 보입니다. 

4. **기존 채널 비동기 정리 후 즉시 재구독**
   `unsubscribe()` 완료를 기다리지 않고 새 채널을 만들면 **동시 다발 채널**이 잠깐 공존합니다. 이때도 CLOSED/TIMED_OUT 로그가 잦아집니다. 

> 참고로, 현재 별도 메시지가 없어도 브라우저 백그라운드·네트워크 요인으로 **TIMED_OUT** 은 나올 수 있는데, 위 충돌 로직이 있으면 *작은 타임아웃 1번*이 **무한 루프**로 증폭됩니다.

또 하나: 대시보드 접근 시 `profiles` RLS 정책 순환으로 **500**이 간헐 발생하면, 상위 컴포넌트 리마운트 → Chat 재마운트 → 같은 루프가 재현될 수 있습니다. **권한 확인을 클라이언트에서 직접 쿼리하지 말고 서버 전용 경로/JWT 클레임으로 분리**하는 것도 재연결 안정화에 중요합니다. 

---

## 바로 적용 가능한 **수정 체크리스트 (소폭 패치)**

> 목표: **수동 재연결을 극단적으로 단순화**하고, **cleanup/타이머/중복 인스턴스 충돌 제거**.

1. **Effect 의존성 다이어트 + Supabase 클라이언트 고정**

* `const supabase = useMemo(() => createClientSupabase(), [])` 처럼 **한 번만 생성/재사용**(또는 모듈 단일톤)
* 구독 effect 의존성에서 `reconnectKey`, `supabase`, `currentUser` 등 **불안정한 값 제거**. **`[webinarId]`만** 두거나, 필요한 값은 **ref**로 읽도록 변경.

2. **채널/타이머 Ref로 관리 & 철저히 clear**

```ts
const channelRef = useRef<RealtimeChannel | null>(null)
const reconnectTimerRef = useRef<ReturnType<typeof setTimeout> | null>(null)
const fallbackTimerRef = useRef<ReturnType<typeof setTimeout> | null>(null)

const clearTimers = () => {
  if (reconnectTimerRef.current) { clearTimeout(reconnectTimerRef.current); reconnectTimerRef.current = null }
  if (fallbackTimerRef.current) { clearTimeout(fallbackTimerRef.current); fallbackTimerRef.current = null }
}
```

* **cleanup**에서 반드시 `clearTimers()` 먼저 호출.

3. **수동 재연결에서 채널을 직접 닫지 말기**

* 재연결은 **“구독 함수 재실행”만** 하세요.
  `setTimeout` 안에서 `channel.unsubscribe()`/`removeChannel()`을 호출하지 말고, **cleanup이 알아서** 처리하게 두면 **중복 정리 레이스**를 피할 수 있습니다. 

```ts
// BAD: setTimeout 내부에서 채널 정리 + 상태 변경
// GOOD: setTimeout에서는 상태 변경만 → effect cleanup가 정리
reconnectTimerRef.current = setTimeout(() => {
  setReconnectKey(prev => prev + 1)  // 또는 내부 connect() 재호출
  reconnectTimerRef.current = null
}, delay)
```

4. **기존 채널 정리 시 `await` 보장**

```ts
const existing = supabase.getChannels().find(ch => ch.topic === `realtime:${channelName}`)
if (existing) {
  await existing.unsubscribe()
  supabase.removeChannel(existing)
  await new Promise(r => setTimeout(r, 100)) // 정리 settle
}
```

* 새 채널 만들기 **전에** 반드시 마무리. 

5. **Chat 단일 인스턴스 원칙**

* 모바일/데스크톱에 같은 컴포넌트를 두 번 렌더하지 말고 **한 번만 렌더**해서 두 컨테이너에 공유(또는 `key`로 명시적 분리). 

6. **폴백 폴링은 “정말 마지막”에만, 그리고 값싼 캐시와 함께**

* 폴링 주기를 10–30초로 늘리고, API 응답에
  `Cache-Control: public, s-maxage=1, stale-while-revalidate=9`를 넣어 **CDN 에지에서 거의 공짜로 응답**되게 구성(서버리스 호출 수 급감).
* 폴링 중에는 **새로운 구독 시도 타이머만** 예약(채널 닫기 금지).

7. **인증 토큰 업데이트만 가볍게**

* `supabase.auth.onAuthStateChange`에서 토큰 바뀔 때 `supabase.realtime.setAuth(token)` 정도만.
* 토큰 갱신은 구독 루프와 분리(불필요한 재구독 금지).

> 위 1–5만 반영해도 **SUBSCRIBED → 즉시 CLOSED 루프**는 멈춥니다. 문제 지점·라인 기준 근거는 첨부 분석에 정리되어 있습니다. 

---

## “그래도 Supabase Realtime이 불안하면?” 대안과 적용 가이드

실시간 설문/퀴즈/추첨은 **지연 1–2초 이내**가 좋고, 대규모 동접에선 **요금/안정성**이 관건입니다.

### 1) **Ably** 또는 **Pusher Channels**

* 장점: 연결 관리·자동 재연결·퍼지/복제 인프라가 안정적. 브라우저/방화벽 친화.
* 단점: 별도 유료 과금(메시지 수 기준).
* 이행: 메시지 전송 시 **서버리스 함수에서 Ably/Pusher REST publish**, 클라에서는 **채널 subscribe**. DB 반영은 비동기(이벤트 수신 시 저장)로 설계하면 WAL 부하↓.

### 2) **Upstash Redis Pub/Sub + SSE**

* 장점: **서버 없는(HTTP) 퍼블리시**, SSE는 프록시 친화. 저렴.
* 단점: 브라우저 탭 다중 연결·재연결 처리 직접 구현 필요.
* 이행: 송신은 API Route → Upstash REST, 수신은 **Next Route Handler(Edge)로 SSE 브리지**. 폴백은 여전히 캐시 친화.

### 3) **Cloudflare Pub/Sub / Durable Objects**

* 장점: 전 세계 에지에서 저지연. **실시간 상태(참여자 수, 추첨 seed)** 공유가 용이.
* 단점: 벤더 종속, 러닝커브.

### 4) **Supabase Realtime 유지 + Broadcast/PRESENCE로 분리**

* “채팅 텍스트”는 `postgres_changes`,
* **투표 카운트/퀴즈 진행/추첨 후보**처럼 **초저지연·고빈도·일시적 데이터는 `broadcast` 채널**로 보내면 DB WAL 부하와 RLS 영향에서 벗어납니다. (나중에 스냅샷만 저장)

> 비용과 구현 복잡도 관점에서 **Ably(또는 Pusher)로 ‘이벤트 전달’만 외주**하고, **최종 기록은 Supabase**가 가장 무난합니다.

---

## 성능·안정성 점검용 짧은 시나리오

1. **빌드(Production) 모드**에서 테스트 (StrictMode 이펙트 2회 실행 방지).
2. 페이지 최초 진입 → 콘솔에 **SUBSCRIBED 1회만** 찍히는지 확인.
3. 30–60초 대기해도 **CLOSED 없이 유지**되는지 확인.
4. 탭 전환/백그라운드에서도 유지(일시적 TIMED_OUT이 나와도 **즉시 루프로 번지지 않음**).
5. 모바일/데스크톱 동시 렌더 제거 후 **구독 수 = 1**인지 `getChannels()`로 확인. 

---

## 최종 정리

* 현재 **폴링이 과하게 돈다**기보다, **재연결·cleanup·중복 인스턴스 충돌**이 **구독을 스스로 종료**시켜 폴링이 켜지는 구조입니다. 위 체크리스트대로 **(1) 타이머/cleanup 정리, (2) 단일 인스턴스, (3) 기존 채널 await 정리, (4) 재연결은 최소화**만 적용해도 **무한 루프는 멈추고** 체감 지연이 크게 줄어듭니다. 
* 별개로 **RLS 순환**은 클라 직접 쿼리를 제거하고 서버/JWT로 권한 판단을 밀어내 안정적으로 막아두세요. 페이지 리마운트 원인을 줄여 Realtime 안정성에도 기여합니다. 

원하시면, 위 체크리스트 기반으로 **변경 diff** 형태(최소 수정 라인)로 정리해 드릴게요.
